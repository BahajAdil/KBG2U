{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb5c8cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from os.path import join\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from pathlib import Path\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from itertools import chain, groupby\n",
    "from typing import Any, List, Optional, Union\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, ComplementNB\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import InputExample, SentenceTransformer, losses\n",
    "from sentence_transformers.losses import BatchHardTripletLossDistanceFunction as LossDistances\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from glob import glob\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "StrOrPath = Union[Path, str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b97889",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params():\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5e7bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d8aed85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0+cu113'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbdc28d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTUALITY_INT = {'Uncommitted':1, 'Fact':2,\n",
    "                  'Probable':3, 'Possible':4, 'Counterfact':5,\n",
    "                  'Doubtful':6, 'Conditional':7}\n",
    "FACTUALITY_INT_REV = {FACTUALITY_INT[i]:i for i in FACTUALITY_INT}\n",
    "MARGIN_LOSSES = ['', '']\n",
    "DISTANCE_LOSSES = ['BatchHardSoftMarginTripletLoss']\n",
    "MARG_DIST_LOSSES = ['BatchAllTripletLoss', 'BatchHardTripletLoss',\n",
    "                    'BatchSemiHardTripletLoss', 'ContrastiveLoss', 'TripletLoss']\n",
    "random_state = 1234\n",
    "FACT_DIR = '/home/pc/Desktop/AdilStuff/Projects/SemRepMed/semmed_data/FactualityData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ab4211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5227e4d350>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff55d09",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa0de9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_lines(file_name,dict_data):\n",
    "    json_string = json.dumps(dict_data)\n",
    "    with open(file_name, 'a') as f:\n",
    "        f.write(json_string+\"\\n\")\n",
    "        \n",
    "def read_json_lines(file_name):\n",
    "    lines = []\n",
    "    with open(file_name) as file_in:\n",
    "        for line in file_in:\n",
    "            lines.append(json.loads(line))\n",
    "    return lines\n",
    "def infer_fact(in_file_name, out_file_name):\n",
    "    args = Params()\n",
    "    # paraphrase-multilingual-mpnet-base-v2 paraphrase-albert-small-v2 all-mpnet-base-v2 all-MiniLM-L6-v2 \n",
    "    # all-MiniLM-L12-v2 \n",
    "    args.model_path = join(FACT_DIR, 'MODELS')\n",
    "    pip = FullPipe(args,\n",
    "        x_train= None,\n",
    "        y_train= None,\n",
    "        x_test= None,\n",
    "        y_test= None, mode = 'inference')\n",
    "    \n",
    "    with open(in_file_name) as file_in:\n",
    "        pbar = tqdm(total = 375866742)\n",
    "        for line in file_in:\n",
    "            lines = []\n",
    "            data = json.loads(line)\n",
    "            PREDICATION_AUX_ID = data['PREDICATION_AUX_ID']\n",
    "            SENTENCE, FORMATED_SENTENCE = data['SENTENCE'], data['FORMATED_SENTENCE']\n",
    "            label = pip.predict([FORMATED_SENTENCE])\n",
    "            label = label[1][0]\n",
    "            res_data = {}\n",
    "            res_data['PREDICATION_AUX_ID'], res_data['LABEL'] = PREDICATION_AUX_ID, label\n",
    "            write_json_lines(file_name = out_file_name, dict_data = res_data)\n",
    "            pbar.update()\n",
    "    return res_data\n",
    "\n",
    "def infer_fact_csv(in_file_name):\n",
    "    args = Params()\n",
    "    # paraphrase-multilingual-mpnet-base-v2 paraphrase-albert-small-v2 all-mpnet-base-v2 all-MiniLM-L6-v2 \n",
    "    # all-MiniLM-L12-v2 \n",
    "    args.model_path = join(FACT_DIR, 'MODELS')\n",
    "    pip = FullPipe(args,\n",
    "        x_train= None,\n",
    "        y_train= None,\n",
    "        x_test= None,\n",
    "        y_test= None, mode = 'inference')\n",
    "    all_paths = glob(join(in_file_name, 'form_sent_*.csv'))\n",
    "    print('file count: ', len(all_paths))\n",
    "    for f_file in tqdm(all_paths):\n",
    "        fil_num = f_file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "        df = pd.read_csv(f_file, compression = 'gzip')\n",
    "        FORMATED_SENTENCE = list(df['FORMATED_SENTENCE'])\n",
    "        PREDICATION_AUX_ID = list(df['PREDICATION_AUX_ID'])\n",
    "        label = pip.predict(FORMATED_SENTENCE)\n",
    "        label = label[1]\n",
    "        df_res = pd.DataFrame([{'PREDICATION_AUX_ID':k,'label':v} for k,v in zip(PREDICATION_AUX_ID, label)])\n",
    "        df_res.to_csv(join(in_file_name, 'labeled_sent_'+fil_num+'.csv'), index = False, compression = 'gzip')\n",
    "        \n",
    "def process_record(line):\n",
    "    print('process')\n",
    "    data = json.loads(line)\n",
    "    PREDICATION_AUX_ID = data['PREDICATION_AUX_ID']\n",
    "    SENTENCE, FORMATED_SENTENCE = data['SENTENCE'], data['FORMATED_SENTENCE']\n",
    "    print(PREDICATION_AUX_ID)\n",
    "\n",
    "def infer_fact_parallel(in_file_name):\n",
    "    numthreads = 8\n",
    "    numlines = 100\n",
    "#     lines = open(in_file_name).readlines()\n",
    "#     r = process_map(_foo, lines, max_workers=numthreads)\n",
    "    pool = ThreadPool(3)\n",
    "    file = open(in_file_name)\n",
    "    print('map')\n",
    "    res = pool.map(process_record,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db363517",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "14a8eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentTrans():\n",
    "    def __init__(self, args, x_train, y_train, x_test, y_test):\n",
    "        self.args = args\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        if hasattr(args, 'train_iter'):\n",
    "            self.train_iter = args.train_iter\n",
    "        if hasattr(args, 'warmup_steps'):\n",
    "            self.warmup_steps = args.warmup_steps\n",
    "        if hasattr(args, 'data_iter'):\n",
    "            self.data_iter = args.data_iter\n",
    "        if hasattr(args, 'n_neg'):\n",
    "            self.n_neg = args.n_neg\n",
    "        if hasattr(args, 'loss_margin'):\n",
    "            self.loss_margin = args.loss_margin\n",
    "        if hasattr(args, 'loss_name'):\n",
    "            self.loss_name = args.loss_name\n",
    "            loss =  get_loss(args.loss_name, args)\n",
    "        else:\n",
    "            self.loss_name = None\n",
    "            loss = None\n",
    "        if hasattr(args, 'loss_distance'):\n",
    "            self.loss_distance = get_distance(args.loss_distance)\n",
    "        if hasattr(args, 'model'):\n",
    "            self.model = SentenceTransformer(args.model)\n",
    "            self.model = self.model.to(device)\n",
    "        elif hasattr(args, 'model_path'):\n",
    "            self.model = SentenceTransformer(args.model_path)\n",
    "            self.model = self.model.to(device)\n",
    "        if loss is not None:\n",
    "            if self.loss_name in MARG_DIST_LOSSES:\n",
    "                self.loss = loss(self.model, self.loss_distance, self.loss_margin)\n",
    "            elif self.loss_name in DISTANCE_LOSSES:\n",
    "                self.loss = loss(self.model, self.loss_distance)\n",
    "            else:\n",
    "                self.loss = loss(self.model)\n",
    "        if self.loss_name is not None:\n",
    "            if 'Triplet' in self.loss_name:\n",
    "    #             train_examples = weighted_generate_multiple_sentence_triples(x_train, y_train, self.data_iter)\n",
    "                train_examples = mult_neg_weighted_generate_multiple_sentence_triples(x_train, y_train, self.n_neg, self.data_iter)\n",
    "            else:\n",
    "    #             train_examples = weighted_generate_multiple_sentence_pairs(self.x_train, self.y_train, self.data_iter)\n",
    "                train_examples = mult_neg_weighted_generate_multiple_sentence_pairs(self.x_train, self.y_train, self.n_neg,self.data_iter)\n",
    "    #         train_examples = generate_multiple_sentence_pairs(self.x_train, self.y_train, self.data_iter)\n",
    "    #         train_examples = generate_multiple_sentence_triples(x_train, y_train, self.data_iter)\n",
    "\n",
    "            self.train_dataloader = DataLoader(\n",
    "                    train_examples,\n",
    "                    shuffle=True,\n",
    "                    batch_size=args.batch_size,\n",
    "                    generator=torch.Generator(device='cpu'),\n",
    "            )\n",
    "\n",
    "#         self.loss = loss(self.model)\n",
    "        if self.x_train is not None:\n",
    "            self.X_train_noFT = self.model.encode(self.x_train)\n",
    "        if self.x_test is not None:\n",
    "            self.X_test_noFT = self.model.encode(self.x_test)\n",
    "        \n",
    "    def fit(self, show_progress_bar=True):\n",
    "        self.model.fit(\n",
    "            train_objectives=[(self.train_dataloader, self.loss)],\n",
    "            epochs=self.train_iter,\n",
    "            warmup_steps=self.warmup_steps,\n",
    "            show_progress_bar=show_progress_bar,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def get_train_test_features(self):\n",
    "        return self.model.encode(self.x_train), self.model.encode(self.x_test)\n",
    "    \n",
    "    def get_embeddings(self, x):\n",
    "        return self.model.encode(x)\n",
    "    \n",
    "    def plot_(self):\n",
    "        plt.figure(figsize=(20,10))\n",
    "\n",
    "        #Plot X_train_noFit\n",
    "        X_embedded = TSNE(init='pca', n_components=2).fit_transform(np.array(self.X_train_noFT))\n",
    "        plt.subplot(221)\n",
    "        plt.title('X_train No Fit')\n",
    "\n",
    "        for i, t in enumerate(set(np.array(self.y_train))):\n",
    "            idx = np.array(self.y_train) == t\n",
    "            plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=FACTUALITY_INT_REV[t])   \n",
    "\n",
    "        plt.legend(bbox_to_anchor=(1, 1));\n",
    "\n",
    "        #Plot X_eval noFit\n",
    "        X_embedded = TSNE(init='pca',n_components=2).fit_transform(np.array(self.X_test_noFT))\n",
    "        plt.subplot(223)\n",
    "        plt.title('X_test No Fit')\n",
    "\n",
    "        for i, t in enumerate(set(np.array(self.y_test))):\n",
    "            idx = np.array(self.y_test) == t\n",
    "            plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=FACTUALITY_INT_REV[t])   \n",
    "\n",
    "        plt.legend(bbox_to_anchor=(1, 1));\n",
    "\n",
    "        X_train, X_test = self.get_train_test_features()\n",
    "        #Plot X_train SetFit\n",
    "        X_embedded = TSNE(init='pca',n_components=2).fit_transform(np.array(X_train))\n",
    "\n",
    "        plt.subplot(222)\n",
    "        plt.title('X_train SetFit')\n",
    "\n",
    "        for i, t in enumerate(set(np.array(self.y_train))):\n",
    "            idx = np.array(self.y_train) == t\n",
    "            plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=FACTUALITY_INT_REV[t])   \n",
    "\n",
    "        plt.legend(bbox_to_anchor=(1, 1));\n",
    "\n",
    "        #Plot X_eval SetFit\n",
    "        X_embedded = TSNE(init='pca',n_components=2).fit_transform(np.array(X_test))\n",
    "        plt.subplot(224)\n",
    "        plt.title('X_test SetFit')\n",
    "\n",
    "        for i, t in enumerate(set(np.array(self.y_test))):\n",
    "            idx = np.array(self.y_test) == t\n",
    "            plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=FACTUALITY_INT_REV[t])   \n",
    "\n",
    "        plt.legend(bbox_to_anchor=(1, 1))\n",
    "        plt.savefig('embedding_distribution.pdf', bbox_inches = 'tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4713fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead():\n",
    "    def __init__(self, args, x_train, y_train, x_test, y_test):\n",
    "        self.args = args\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.classifier = args.classifier\n",
    "        self.transformation = args.transformation\n",
    "        self.classifier = get_classifier_head(ch_name = self.classifier)\n",
    "        self.class_fitted = False\n",
    "        \n",
    "    def transform(self, x):\n",
    "        if self.transformation == 'normalize':\n",
    "            return preprocessing.normalize(x, norm='l2')\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def fit(self):\n",
    "        self.classifier.fit(self.x_train, self.y_train)\n",
    "        self.class_fitted = True\n",
    "        \n",
    "    def fit_transform(self):\n",
    "        self.x_train = self.transform(self.x_train)\n",
    "        self.x_test = self.transform(self.x_test)\n",
    "        self.fit()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        preds = self.classifier.predict(x)\n",
    "        return preds\n",
    "    \n",
    "    def report_test(self, output_dict = False):\n",
    "        if self.class_fitted:\n",
    "            test_peds = self.predict(self.x_test)\n",
    "            report = classification_report(self.y_test, test_peds,\n",
    "                                            labels = list(FACTUALITY_INT.values()),\n",
    "                                            output_dict=output_dict,\n",
    "                                            target_names=list(FACTUALITY_INT.keys()),\n",
    "                                            zero_division = True)\n",
    "            print(report)\n",
    "    def confusion_matrix(self):\n",
    "        if self.class_fitted:\n",
    "            predictions = self.predict(self.x_test)\n",
    "#             cm = confusion_matrix(self.y_test, predictions, display_labels=list(FACTUALITY_INT.values()),\n",
    "#                                   normalize='true')\n",
    "#             disp = ConfusionMatrixDisplay.from_predictions(confusion_matrix=cm,\n",
    "#                                           display_labels=list(FACTUALITY_INT.keys()))\n",
    "            disp = ConfusionMatrixDisplay.from_predictions(\n",
    "                self.y_test, predictions,labels = list(FACTUALITY_INT.values()),\n",
    "                display_labels=list(FACTUALITY_INT.keys()), xticks_rotation = 45, normalize= 'true'\n",
    "            )\n",
    "#             disp.plot()\n",
    "            plt.savefig('confusion_matrix.pdf', bbox_inches = 'tight')\n",
    "            plt.show()\n",
    "#             skplt.metrics.plot_confusion_matrix(self.y_test, predictions, x_tick_rotation=45,\n",
    "#                                                 labels=list(FACTUALITY_INT.keys()), normalize=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a4cd6aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullPipe():\n",
    "    def __init__(self, args, x_train, y_train, x_test, y_test, mode='train'):\n",
    "        random.seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "        torch.manual_seed(random_state)\n",
    "        if mode == 'train':\n",
    "            self.strans = SentTrans(args, x_train, y_train, x_test, y_test)\n",
    "            self.args = args\n",
    "            self.strans_fitted = False\n",
    "            self.class_model_fitted = False\n",
    "        elif mode == 'inference':\n",
    "            self.strans = SentTrans(args, None, None, None, None)\n",
    "            self.class_model = joblib.load(Path(args.model_path) / \"classifier.pkl\")\n",
    "            self.strans_fitted = True\n",
    "            self.class_model_fitted = True\n",
    "            \n",
    "    def fit(self):\n",
    "        self.strans.fit()\n",
    "        self.strans_fitted = True\n",
    "        x_train, x_test = self.strans.get_train_test_features()\n",
    "        y_train, y_test = self.strans.y_train, self.strans.y_test\n",
    "        self.class_model = ClassificationHead(self.args, x_train, y_train, x_test, y_test)\n",
    "        self.class_model.fit()\n",
    "        self.class_model_fitted = True\n",
    "        \n",
    "    def predict(self, x, y = None):\n",
    "        if self.strans_fitted & self.class_model_fitted:\n",
    "            x = self.strans.model.encode(x)\n",
    "            if len(x.shape) == 1:\n",
    "                x = x.reshape(1, -1)\n",
    "#             print(x.shape)\n",
    "#             print(type(x))\n",
    "            preds = self.class_model.predict(x)\n",
    "            return preds, [FACTUALITY_INT_REV[i] for i in list(preds)], y\n",
    "        else:\n",
    "            print('The models should be fitted')\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.strans.plot_()\n",
    "        self.class_model.report_test()\n",
    "        self.class_model.confusion_matrix()\n",
    "        \n",
    "    def save(\n",
    "        self,\n",
    "        path: StrOrPath,\n",
    "        model_name: Optional[str] = None,\n",
    "        create_model_card: bool = False,\n",
    "    ):\n",
    "        if (not self.class_model_fitted) and (not self.strans_fitted):\n",
    "            raise NotFittedError(\n",
    "                \"This SetFitClassifier instance is not fitted yet.\"\n",
    "                \" Call 'fit' with appropriate arguments before saving this estimator.\"\n",
    "            )\n",
    "        self.strans.model.save(str(path), self.args.model, create_model_card)\n",
    "        joblib.dump(self.class_model.classifier, Path(path) / \"classifier.pkl\")\n",
    "\n",
    "    def load(self, cls, path: StrOrPath):\n",
    "        args.model = path\n",
    "        self.strans = SentTrans(args, None, None, None, None)\n",
    "        self.class_model = joblib.load(Path(path) / \"classifier.pkl\")\n",
    "        return setfit\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb53cbde",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b8121065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer_fact(in_file_name = 'all_sentences.jsonl', out_file_name = 'all_sentences_facts.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4b713669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file count:  1126785\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1629e31700432c8122c6218561ce03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1126785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = infer_fact_csv('Format_sents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "62e8e11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1126785"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob(join('Format_sents', 'labeled_sent_*.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1383cdb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PREDICATION_AUX_ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167307222</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167307223</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>167307224</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>167307225</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>167307226</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>167307319</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>167307320</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>167307321</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>167307322</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>167307323</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PREDICATION_AUX_ID label\n",
       "0             167307222  Fact\n",
       "1             167307223  Fact\n",
       "2             167307224  Fact\n",
       "3             167307225  Fact\n",
       "4             167307226  Fact\n",
       "..                  ...   ...\n",
       "96            167307319  Fact\n",
       "97            167307320  Fact\n",
       "98            167307321  Fact\n",
       "99            167307322  Fact\n",
       "100           167307323  Fact\n",
       "\n",
       "[101 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('Format_sents/labeled_sent_849503.csv',compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a626776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PREDICATION_AUX_ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167307222</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167307223</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>167307224</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>167307225</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>167307226</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>167307319</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>167307320</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>167307321</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>167307322</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>167307323</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PREDICATION_AUX_ID label\n",
       "0             167307222  Fact\n",
       "1             167307223  Fact\n",
       "2             167307224  Fact\n",
       "3             167307225  Fact\n",
       "4             167307226  Fact\n",
       "..                  ...   ...\n",
       "96            167307319  Fact\n",
       "97            167307320  Fact\n",
       "98            167307321  Fact\n",
       "99            167307322  Fact\n",
       "100           167307323  Fact\n",
       "\n",
       "[101 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e063bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PREDICATION_AUX_ID</th>\n",
       "      <th>SENTENCE</th>\n",
       "      <th>FORMATED_SENTENCE</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167307222</td>\n",
       "      <td>BACKGROUND: Two and a half years after commenc...</td>\n",
       "      <td>BACKGROUND: Two and a half years after commenc...</td>\n",
       "      <td>SENTENCE/split_990577.csv.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167307223</td>\n",
       "      <td>BACKGROUND: Two and a half years after commenc...</td>\n",
       "      <td>BACKGROUND: Two and a half years after commenc...</td>\n",
       "      <td>SENTENCE/split_990577.csv.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>167307224</td>\n",
       "      <td>This subsequent outbreak provided the opportun...</td>\n",
       "      <td>This subsequent outbreak provided the opportun...</td>\n",
       "      <td>SENTENCE/split_990577.csv.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>167307225</td>\n",
       "      <td>Children with rotavirus-confirmed gastroenteri...</td>\n",
       "      <td>@PREDICAT$ @OBJECT$ rotavirus-confirmed @SUBJE...</td>\n",
       "      <td>SENTENCE/split_990577.csv.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>167307226</td>\n",
       "      <td>Nineteen (46%) of 41 case patients had receive...</td>\n",
       "      <td>Nineteen (46%) of 41 case patients had receive...</td>\n",
       "      <td>SENTENCE/split_990577.csv.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>167307319</td>\n",
       "      <td>However, the subtype PsA was more prevalent in...</td>\n",
       "      <td>However, the subtype PsA was more prevalent in...</td>\n",
       "      <td>SENTENCE/split_990577.csv.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>167307320</td>\n",
       "      <td>CONCLUSION: In Sweden the prevalence of spondy...</td>\n",
       "      <td>CONCLUSION: In Sweden the prevalence of @SUBJE...</td>\n",
       "      <td>SENTENCE/split_990577.csv.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>167307321</td>\n",
       "      <td>PsA was the most frequent subtype followed by ...</td>\n",
       "      <td>@SUBJECT$ was the most frequent subtype @OBJEC...</td>\n",
       "      <td>SENTENCE/split_990577.csv.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>167307322</td>\n",
       "      <td>Magnetic resonance imaging of skeletal muscles...</td>\n",
       "      <td>@PREDICAT$ @OBJECT$ @SUBJECT$ in sporadic incl...</td>\n",
       "      <td>SENTENCE/split_990577.csv.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>167307323</td>\n",
       "      <td>Magnetic resonance imaging of skeletal muscles...</td>\n",
       "      <td>@SUBJECT$ of skeletal muscles @OBJECT$ @PREDIC...</td>\n",
       "      <td>SENTENCE/split_990577.csv.gz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PREDICATION_AUX_ID                                           SENTENCE  \\\n",
       "0             167307222  BACKGROUND: Two and a half years after commenc...   \n",
       "1             167307223  BACKGROUND: Two and a half years after commenc...   \n",
       "2             167307224  This subsequent outbreak provided the opportun...   \n",
       "3             167307225  Children with rotavirus-confirmed gastroenteri...   \n",
       "4             167307226  Nineteen (46%) of 41 case patients had receive...   \n",
       "..                  ...                                                ...   \n",
       "96            167307319  However, the subtype PsA was more prevalent in...   \n",
       "97            167307320  CONCLUSION: In Sweden the prevalence of spondy...   \n",
       "98            167307321  PsA was the most frequent subtype followed by ...   \n",
       "99            167307322  Magnetic resonance imaging of skeletal muscles...   \n",
       "100           167307323  Magnetic resonance imaging of skeletal muscles...   \n",
       "\n",
       "                                     FORMATED_SENTENCE  \\\n",
       "0    BACKGROUND: Two and a half years after commenc...   \n",
       "1    BACKGROUND: Two and a half years after commenc...   \n",
       "2    This subsequent outbreak provided the opportun...   \n",
       "3    @PREDICAT$ @OBJECT$ rotavirus-confirmed @SUBJE...   \n",
       "4    Nineteen (46%) of 41 case patients had receive...   \n",
       "..                                                 ...   \n",
       "96   However, the subtype PsA was more prevalent in...   \n",
       "97   CONCLUSION: In Sweden the prevalence of @SUBJE...   \n",
       "98   @SUBJECT$ was the most frequent subtype @OBJEC...   \n",
       "99   @PREDICAT$ @OBJECT$ @SUBJECT$ in sporadic incl...   \n",
       "100  @SUBJECT$ of skeletal muscles @OBJECT$ @PREDIC...   \n",
       "\n",
       "                        file_name  \n",
       "0    SENTENCE/split_990577.csv.gz  \n",
       "1    SENTENCE/split_990577.csv.gz  \n",
       "2    SENTENCE/split_990577.csv.gz  \n",
       "3    SENTENCE/split_990577.csv.gz  \n",
       "4    SENTENCE/split_990577.csv.gz  \n",
       "..                            ...  \n",
       "96   SENTENCE/split_990577.csv.gz  \n",
       "97   SENTENCE/split_990577.csv.gz  \n",
       "98   SENTENCE/split_990577.csv.gz  \n",
       "99   SENTENCE/split_990577.csv.gz  \n",
       "100  SENTENCE/split_990577.csv.gz  \n",
       "\n",
       "[101 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cb51ca",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f65c9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minfer_fact_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_file_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall_sentences.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36minfer_fact_parallel\u001b[0;34m(in_file_name)\u001b[0m\n\u001b[1;32m     50\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(in_file_name)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_record\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/TestEnv3/lib/python3.8/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/anaconda3/envs/TestEnv3/lib/python3.8/multiprocessing/pool.py:475\u001b[0m, in \u001b[0;36mPool._map_async\u001b[0;34m(self, func, iterable, mapper, chunksize, callback, error_callback)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_running()\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(iterable, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 475\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m     chunksize, extra \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdivmod\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/TestEnv3/lib/python3.8/codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "infer_fact_parallel(in_file_name = 'all_sentences.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25cb01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.contrib.concurrent import process_map  # or thread_map\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5301bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _foo(my_number):\n",
    "    square = my_number * my_number\n",
    "    time.sleep(1)\n",
    "    return square \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "189ad465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26809427be684033baafa44beed258be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = process_map(_foo, range(0, 30), max_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d00cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
